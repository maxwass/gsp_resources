Background:

Commutative Matrices := Two matrices A,B commute if AB = BA
Hermitian Matrix     := Self-Adjoint Matrix := square matrix A st A^H = A
    -A^H is the conjugate transpose of A or the 'hermitian tranpose of A'

Normal Matrix := complex square matrix st U^H*U = U*U^H
    -normal matrices commute with their conjugate transpose
    Subset of Normal Matrices:
        Unitary Matrix    := complex square matrix st U^H*U = U*U^H = I
        Orthogonal Matrix := real square matrix    st U^T*U = U*U^T = I

Characteristic Polynomial of a square matrix A: p_a(x) = det(A-xI)
Minimal Polynomial of a square matrix A: m_a(x) = polynomial of least degree
    such that m_a(A) = 0

When does characteristic = minimal?
    When all eigenvalues distinct <-> charact. polyn only has simple roots

Theorem 0: If - BA==AB
 [1]            &                   ===>   B = h(A) where h is a polynomial of deg(m_a(x))
            - A's has dist eigvals

Theorem 1: If A diagonalizable ===> any polynomial h(A) can be written as
                                  h(A) = V * diag(h(l_0) ,..., h(l_N-1)) * V^-1
                                  where - l_i is eigenvalue of A
                                        - V is matrix of eigvec's of A

Theorem 2:
    Let A=VUV^-1 be a diagonalizable matrix
    Let H=h(A) be some polynomial of A
    Then H has same eigenvectors as A.
pf:
    H = h(A) = h(VUV^-1) = Vh(U)V^-1,
    where h(U) = diag(h(l0),h(l1)...) where li are eigevalues of A


Remark: Unitary Diagonalizable vs Diagonalizable
    - Diagonalizable         := A = VUV^-1
    - Unitary Diagonalizable := A = VUV^H

If A is Normal ==> There exists complete set of orthonormal eigenvectors
               ==> A is Unitarily Diagonalizable
               ==> A = VUV^H
[2]

[1] https://en.wikipedia.org/wiki/Commuting_matrices
[2] https://math.stackexchange.com/questions/778946/prove-that-if-a-is-normal-then-eigenvectors-corresponding-to-distinct-eigenva



Question:
What breaks down when a vertex basis is chosen which is no the graph fourier basis = eigvecs of A

Show A is local operation on s -> H = h(A) can be impleneted in vertex domain
 - do when dont want to take eigendecomp of A

Notes:

Outline:
    -review concept of shift in DSP
    -develop corresponding notion of shift for GSP
    -this leads to defn of frequencies for graph signals and their interpretations

2: KEY INGREDIENTS OF GRAPH SIGNAL PROCESSING
A) Recall: DSP investigates...
    a) signals and their representations
    b) systems that process signals, usually referred to as filters
    c) signal transforms, including two very important ones, namely, the z-transform and the Fourier transform
    d) sampling of signals, as well as other more specialized topics.

    restrict ourselves to finite signals, FIR filters

    signals:
        s = (s_0,s_1,...,s_N-1)
        z transform of s : s(z) = sum s[i]*z^-i, i = 0->N-1
        DFT of s:      s_hat[k] = <s,e_k,N>, where e_kn is complex exponential with freq 2*pi*k/N
        iDFT of s_hat: s = sum s_hat[i]*(e_i,N), i = 0->N-1
    filters: An FIR filter is also represented by a polynomial in z^−1 ('shifts' or 'delays')
        h = sum h_n*z^-n, n:0->N-1, h_n in R
        s_out(t) = conv( h(t) , s_in(t) )
        s_out(z) =       h(z) * s_in(z)

        Only considering finite time signals
        -> h(z) * s_in(z) could result in s_out(z) being a polynomial in z−1 of degree greater than N − 1
        -> we have to consider bound- ary conditions (b.c.).
        -> for simplicity, we consider periodic extensions of the signal,
            > sn = s_(n % N) . In other words, the real line is folded around the circle.

        prop 1) filters are polynomials in the shift z^-1
        prop 2) shift invariance (the shift is commutative with the filter)
            -because filters are polynomials in the shift: z^-1 * h(z) = h(z) * z^-1
            "delaying the input signal s_in and then filtering the delayed input signal leads to the same signal
              as first filtering the input signal sin and then delaying the filtered output."

        ** Highlights:
            1) represent signals by (finite degree) polynomials in z^−1
            2) and build filters also as polynomials in z^−1.

B) Defining shifts in Graph Signal Processing
    signal now vector s, filter now matrix H
    In particular, the ~~shift filtering operation corresponds to multiplication by a circulant matrix Ac~~
    A graph interpretation for the DSP concepts of Section II-A can be achieved by viewing the 0-1 shift matrix
        Ac as the adjacency matrix of a graph
    **Dual role of the matrix Ac: represents both the
        1) shift z^−1 in DSP and
        2) the adjacency matrix of the associated time graph
    Thus this circulant case acts as the foundation on which to generalize DSP concepts
        -for regular DSP Ac simply acts as the shift
        -for GSP Ac is interpreted as the adj matrix of the graph on which the signal lives
    More evidence that this is a good idea:
        -taking the eigendecomp of Ac: Ac = VUV^-1 gives us the standard fourier basis as eigenvectors!
        -the classical fourier transform is simply V^-1!!!!!

    Thus interpret the Adjacency Matrix of a graph signal as the 'shift'

    Lets try to define filters to satisfy the 2 properties above: shift invariance and polynomial filters
        We restrict ourselves to matrices H which commute with the shift(adj matrix)
            -> HA=AH
        Theorem: Let A have N distinct eigenvectors and let V be an NxN matrix with its columns as A's
                 eigenvectors. Let H <- V * diag(b0,b1,...,bN-1) * V^-1. Then H commutes with A.
        pf: HA = V diag(b0*l0, b1*l1,...,bN-1*lN-1) V^-1 = AH, where li's are eigenvalues of A. QED.

        To implement a filter in node space, solve system of equations:
            h(l0) = b0, h(l1) = b1,..., h(lN-1) = bN-1, where h is the polynomial in H=h(A).
            Then s_out = H*s_in = (h0 + h1*A + h2*A^2 + ... + hN-1*A^N-1)*s_in

        Theorem 0 (from background):
            If AH==HA && pa(z)==ma(z) THEN H=h(A)
                -where h is a polynomial of at most deg(ma(z))
        Note: all eigenvalues distinct --> pa(z)==ma(z)

        For simplicity: in this discussion we assume A has N distinct eigenvalues and hence a complete
                        set of eigenvectors

C) Frequency representations for graph signals
    In DSP and in linear systems, interested in signals that are invariant when processed by a
        (linear) filter ( h·sin = αsin, where α is a scalar). Theses sin are the eigensignals
        of the filter h. These sin are complex exponentions. We refer to this as the invarience property
        of complex exponentials wrt linear systems.
    In GSP filters are matrices -> eigensignals are eigenvectors. By Theorem 2 in background, the
        eigensignals (eigenfunctions) are the eigenvectors of the shift (adj matrix) A. Thus eigenvectors
        of A are invariant wrt graph filters (Hv = αv).
    Graph Fourier Transforms
        Generalize from the eigendecomp of Ac: Ac = VUV^-1
        It happens to be that V = DFT^-1, U = diag(eigenvals of Ac), V^-1 = DFT
        where the columns of DFT are the complex exponentials (the classic fourier basis!).

        Thus define the Graph Fourier Transform F := V^-1, where A=VUV^-1
            eigenvectors of shift A <- 'graph spectral components'
            eigenvalues of shift A  <- 'graph frequencies'

        Fourier Analysis:  s_hat = Fs = V^-1*s
        Fourier Synthesis: s     = F^-1*s_hat = V*s_hat = sum <vi,s>*vi, i=0->N-1

D) Interpreting Graph Frequencies
    We can now interpret filtering a graph signal:
        s_filtered = Hs
                   = V*diag(b0,..bN-1)*V^-1*s                <- graph fourier transform
                   = V*diag(b0,..bN-1)*s_hat                 <- filtering in graph Fourier space
                   = V*diag(b0*s_hat[0],...,bN-1*s_hat[N-1]) <- inverse fourier transform takes
                                                                 us back to graph node domain

    In DSP (time domain):
        Frequencies are DEFINED from the eigenvalues of the cyclic shift Ac
        f_k := ln(eigval_k) / (-2pi*j)        <- 2017 version includes, 2018 does not
             = ln(exp(-2pi*j*k/N)) / (-2pi*j)
             = 2*pi*k/N,    k = 0->N-1.
        ^ directly related to the degree of variation of the spectral
            components (complex exponentials)
        nice one-to-one correspondence between the ordered value of the frequency and the
            corresponding degree of variation or complexity of the time spectral component.
    In GSP:
        Frequencies are DEFINED by the eigenvalues of the shift
        Frequencies can be complex -> no natural ordering like real numbers
        We lose the nice ordering that exists in the time domain. the magnitude ||eigenvalue||
            does NOT correspond to the variation of the corresponding spectral component (eigenvector)
        Thus we create a metric to order them apppropriately ==> Total Variation
            Total Variation can be defined a few ways
            ex) TV(v_k) := || v_k - A_norm*v_k || where ||-|| is the L1 norm, A_norm <- 1/(eigval_max)*A
            Then using the TV of each eigenvector, we sort the corresponding eigenvalues ('graph frequencies')
            Thus graph frequency lm is larger than graph frequency li if TV(vm)>TV(li)
        Once we create this ordering ^, the definitions of low-band-high pass filters become trivial


E) Frequency representations based on the Laplacian
    We can do the same things using the Laplacian on an undirected graph
    Lalacian is positive semidefinite ==> - all eigenvalues are real & nonnegative
                                          - complete set of orthogonal eigenvectors
    B/c eigenvals are real -> provide natural ordering to their associated eigenvectors
                              in terms of frequency (variation of values over graph)
    We can view the eigenvalue/eigenvector pair as successive optimizers of Rayleigh quotient
        where the kth pair (l_k,u_k) is the solution to:
            argmin x^T*L*x
            -for ||x||=1 in Rn
            -x orthog to previous solns u_0,...,u_k-1

        Thus the kth eigenvalue can be interpreted as the ith graph frequency corresponding
            to the eigenvector u_k

F) Implementation
    How to implement graph filters?
    1) Follow the definition: H = V diag(b0,...,bN-1) V^-1
        Input: b0,...,bN-1 wieghts to scale graph freq's
         i) compute SVD of A to get V                O(N^3)
        ii) explicitly apply to input signal s_in    O(N^3)
            ->s_out = V diag(b0,...,bN-1) V^-1 s_in -------
                                                     O(N^3)

    2) Partial SVD can also be used if the filter h should only be eval'd on top/bottom eigenvals
    For large graphs, better to avoid even partial SVD
    3) Chebyshev approx to H -> plug in power of A into approx -> avoid SVD altogher
        -Does NOT require knowing the eigenvalues and eigenvectors associated to the graph A
        -This also allows us to implement the filter locally
            -A   is a 1 hop operation <-> s_out is weighted sum of nieghbors
            -A^2 is a 2 hop operation <-> s_out is a weighted sum of vertices that are its
                                           neighbors or its neighbors nieghbors
            -A^i is a k hop operation <-> ....
        -Thus, we can traverse the graph vertex-by-vertex, and compute filtered output with only
            accesing its k-hop neighborhood. This is necesessary for VERY large graphs.

3) STATE-OF-THE-ART AND CHALLENGES

A) Frequency Defn
    For Undirected Graphs, guarunteed existance of orthogonal basis.
        Once graph is fixed, frequency is a function of
             i) selected shift operator and its normalization
            ii) etc...
        Making choice appropriately is open research question
        ex^) Suppose the there is high multiplicity of eigenvalues of shift (adj or laplacian)
              -Graph of N nodes has < N unique frequencies
              -Problem: can choose any set of orthogonal vectors within the subspace
                        corresponding to this frequency, leading to different GFTs
                        and thus potentially irreproducible results
              -Soln: Open problem. Proposed soln in paper.
    For Directed Graphs, full set of eigenvectors may not even exist
        We usually restrict ourselves to cases where the adj matrix is invertible and
            eigenvectors do exist

    Recent Work has considered alternative defn's of freq
        i) use random walk laplacian normalization: L_norm = D^-1*L
        ii) explicit optimization to choose a set of graph frequencies

    Best approach to defn freqs for graphs in specific application remains = open question

B) Representations

C) Sampling
    Problem of sampling on graphs is modeled on corresponding problem in DSP
    Basic Idea:
         i) Define class of signals
             DSP ex) signals that are bandlimited to first K freqs of the DFT
             GSP ex) signals that are bandlimited to first K freqs of the GFT
        ii) Define necessary and sufficient conditons to reconstruct signals
                in that class from its samples

    Key Difference in DSP vs GSP sampling => lack of regular sampling patterns in GSP
    This^ prevents us from defining the idea of sampling 'every other node'

    Thus multiple approaches have been suggested to identify the most informative vertices
        on a graph, so that these can be sampled.
    In practice signals usually dont truly belong to the class they are assumed to (e.g.
        signals usually arent truly bandlimmted), and their inclusion in the class cannot
        be guarunteed. Thus the observed signals will be noisy and in general will not
        belong to the pre-specified class
    To address this^ problem, several methods approach the problem of sampling set selection
        from an experiment design perspective.
            Goal: identify set of vertices that minimizes some measure of worst case
                   reconstruction error in cases where noise or model mismatch is present.


D) Extending conventional signal processing to graph signals

E) Graph Learning
    We have thus far assumed the graph is given, or can be defined in a reasonabe way based
    on the nature of the application
        ex) communications network       <--> connectivity defines graph
        ex) temperature sensor network   <--> edge weight is dec func of phyical  distance
        ex) ML applications              <--> edge weight is dec func of distance in feature space

    Recent work: goal is to learn graphs from data
        Motivating scenario:
         i) no reasonable initial graph exists OR
        ii) it is desirable to modify a known graph (based on network connectivity for example)
            by selecting weights derived from data
        KEY IDEA: select a graph such that the most likely graph signals correspond to the lowest
                  frequencies of the GFT
    Remaining Challenges:
    B/c graphs derived from data are essentially models, the 'right' graph model should be selected
        based on
        i) the # of parameters used
        ii) its data fit
        iii) its ability to provide useful interpretations

4) APPLICATIONS
A) Sensor Networks
    graph ~ realtive positions of sensors in the environment
    applications: compression, denoising, reconstruction, distrib processing of sensor data

    first approach to define a graph associted to a sensor network
        -choose edge weights as decreasing funcs of distance btwn nodes
        -then data observatsions that are similar at neighborings nodes lead naturally to smooth (low freq)
            graph signals
        -Such a smooth graph signal model makes it possible to detece outliers or abnormal values by
            high pass filtering & thresholding
    another scenario: graphs to be used for data analysis
    ex) urban data processing relies on data that naturally live on networks (energy, transport, road, etc)
    In some cases relations btwn sensor readins are NOT exclusively explained by the distance between sensor
        locations, or by some actual networks contraint. Other factors can influence that data values observed
        at the sensor readings (think about weather data and a mountain range separating sensors OR measuring
        pollution in a city and accounting for fact that some sensors are near freeways)

D) Machine Learning and Data Science
    Graph methods have played important role in ML applications
        -provide natural way to represent the structure of the data set
        -each vertex represents one data point to which a label can be associated
        -edge weights = decreasing function of the distance between data points in the feature space
    GSP then enables different types of processing, learning, or filtering operations on values associated to
        graph vertices
    In a different context, GSP elements can be helpfu to cnstruct architectures that are able to classify
        signals that live on irregular structures
    examples:
    When data labels are prescribed as signals on a graph, graph signal regularization techniques can be used
        -in the process of estimating labels
        -optimizing the prediction of unkown labels in classification
        -or semi-supervised learning problems
    Graphs can also be constructed to describe the similarities between users or items in reccomendation systems
        Leveraging notions of graph frequency and graph filters, classical collaborative filtering methods
        (like k nearest neighbors strategies) can then be implemented with specific band-stop graph filters
        on graphs
    GSP framework can also be used to design architectures to analyze or classify whole graph signals that
        orignally live on irregular structures
    GSP toolbox used to extend CNN techniques to data defined on graphs
    The CNN paradigm has been generalized with the help of GSP elements for the extraction of feature
        descriptors for 3D shapes
    While previous works mostly address the analyiss of 3D shapes, CNN's can actually be extended to many
        other signals in high dimensional irregular domains
            -social networks, brain connectomes, and word embeddings
        by reformulation in the context of spectral graph theory.
    Here^ the GSP framework leads to fast localized convolutional filters on graphs along witha adaptive pooling
        operators
    Deep network architechtures for graph signals have been tested in various application domains such as
        - chemical molecule properties prediction
        - classification tasks on social networks
        - autism spectrum disorder classification
        - traffic forecasting






Quotes:

"In particular, the graph Fourier transform allows us to develop the intuition gathered in the classical setting and extend it to graphs; we can talk about the notions of frequency and bandlimitedness, for example."

"The machine learning community also con- siders graph structure/data and at times, uses similar methods as GSP does. For example, the graph Fourier basis is related to Laplacian eigenvectors and graph signal recovery is related to semi-supervised learning with graphs[5]. There are, however, a few differences.
    (1) GSP defines a framework that allows the extension of classical signal processing concepts
    (2) Sampling data associated with graphs is rarely studied in the machine learning community and the sampling problem on graphs is generally a hard one; graph-structured data representations (such as graph filter banks and graph dictionary learning) are also rarely studied in the machine learning community;
    (3) Given that GSP extends classical signal processing concepts, it is in a position to consider low-level processing such as denoising, inpainting and compression; and
    (4) Machine learning typically considers a graph as a discrete version of a manifold. In many real-world applications that are associated with graphs, this assumption is however not true. On the contrary, GSP does not make this assumption. For example, there is no underlying manifold for online social networks."

"...GSP has strong connections to several theoretical and practical research domains; its promise lies in its ability to develop new tools and approach existing problems from different perspectives."

